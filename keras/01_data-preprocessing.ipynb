{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for file-management\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = False\n",
    "participants = list(range(1, 21))\n",
    "smartphones = [\"S3Mini\", \"S4\", \"N5X\", \"N6\"]\n",
    "event_time_diff = 3\n",
    "normalize_screen = True\n",
    "\n",
    "sensors = [\n",
    "    \"acc\",\n",
    "    \"gyro\",\n",
    "    \"ori\",\n",
    "    \"grav\",\n",
    "    \"mag\",\n",
    "    \"rot\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"points\",\n",
    "    \"fitts\"\n",
    "]\n",
    "\n",
    "file_names = sensors + tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing - participant: 1 smartphone: S3Mini\n",
      "processing - participant: 1 smartphone: S4\n",
      "processing - participant: 1 smartphone: N5X\n",
      "processing - participant: 1 smartphone: N6\n",
      "processing - participant: 2 smartphone: S3Mini\n",
      "processing - participant: 2 smartphone: S4\n",
      "processing - participant: 2 smartphone: N5X\n",
      "processing - participant: 2 smartphone: N6\n",
      "processing - participant: 3 smartphone: S3Mini\n",
      "processing - participant: 3 smartphone: S4\n",
      "processing - participant: 3 smartphone: N5X\n",
      "processing - participant: 3 smartphone: N6\n",
      "processing - participant: 4 smartphone: S3Mini\n",
      "processing - participant: 4 smartphone: S4\n",
      "processing - participant: 4 smartphone: N5X\n",
      "processing - participant: 4 smartphone: N6\n",
      "processing - participant: 5 smartphone: S3Mini\n",
      "processing - participant: 5 smartphone: S4\n",
      "processing - participant: 5 smartphone: N5X\n",
      "processing - participant: 5 smartphone: N6\n",
      "processing - participant: 6 smartphone: S3Mini\n",
      "processing - participant: 6 smartphone: S4\n",
      "processing - participant: 6 smartphone: N5X\n",
      "processing - participant: 6 smartphone: N6\n",
      "processing - participant: 7 smartphone: S3Mini\n",
      "processing - participant: 7 smartphone: S4\n",
      "processing - participant: 7 smartphone: N5X\n",
      "processing - participant: 7 smartphone: N6\n",
      "processing - participant: 8 smartphone: S3Mini\n",
      "processing - participant: 8 smartphone: S4\n",
      "processing - participant: 8 smartphone: N5X\n",
      "processing - participant: 8 smartphone: N6\n",
      "processing - participant: 9 smartphone: S3Mini\n",
      "processing - participant: 9 smartphone: S4\n",
      "processing - participant: 9 smartphone: N5X\n",
      "processing - participant: 9 smartphone: N6\n",
      "processing - participant: 10 smartphone: S3Mini\n",
      "processing - participant: 10 smartphone: S4\n",
      "processing - participant: 10 smartphone: N5X\n",
      "processing - participant: 10 smartphone: N6\n",
      "processing - participant: 11 smartphone: S3Mini\n",
      "processing - participant: 11 smartphone: S4\n",
      "processing - participant: 11 smartphone: N5X\n",
      "processing - participant: 11 smartphone: N6\n",
      "processing - participant: 12 smartphone: S3Mini\n",
      "processing - participant: 12 smartphone: S4\n",
      "processing - participant: 12 smartphone: N5X\n",
      "processing - participant: 12 smartphone: N6\n",
      "processing - participant: 13 smartphone: S3Mini\n",
      "processing - participant: 13 smartphone: S4\n",
      "processing - participant: 13 smartphone: N5X\n",
      "processing - participant: 13 smartphone: N6\n",
      "processing - participant: 14 smartphone: S3Mini\n",
      "processing - participant: 14 smartphone: S4\n",
      "processing - participant: 14 smartphone: N5X\n",
      "processing - participant: 14 smartphone: N6\n",
      "processing - participant: 15 smartphone: S3Mini\n",
      "processing - participant: 15 smartphone: S4\n",
      "processing - participant: 15 smartphone: N5X\n",
      "processing - participant: 15 smartphone: N6\n",
      "processing - participant: 16 smartphone: S3Mini\n",
      "processing - participant: 16 smartphone: S4\n",
      "processing - participant: 16 smartphone: N5X\n",
      "processing - participant: 16 smartphone: N6\n",
      "processing - participant: 17 smartphone: S3Mini\n",
      "processing - participant: 17 smartphone: S4\n",
      "processing - participant: 17 smartphone: N5X\n",
      "processing - participant: 17 smartphone: N6\n",
      "processing - participant: 18 smartphone: S3Mini\n",
      "processing - participant: 18 smartphone: S4\n",
      "processing - participant: 18 smartphone: N5X\n",
      "processing - participant: 18 smartphone: N6\n",
      "processing - participant: 19 smartphone: S3Mini\n",
      "processing - participant: 19 smartphone: S4\n",
      "processing - participant: 19 smartphone: N5X\n",
      "processing - participant: 19 smartphone: N6\n",
      "processing - participant: 20 smartphone: S3Mini\n",
      "processing - participant: 20 smartphone: S4\n",
      "processing - participant: 20 smartphone: N5X\n",
      "processing - participant: 20 smartphone: N6\n",
      "CPU times: user 8h 15min 17s, sys: 1min 16s, total: 8h 16min 34s\n",
      "Wall time: 8h 16min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for PID in participants:\n",
    "    for smartphone in smartphones:\n",
    "        print(\"processing - participant:\", str(PID), \"smartphone:\", smartphone)\n",
    "\n",
    "        # set screen resolution\n",
    "        if smartphone == \"N5X\":\n",
    "            pixels = {\"width\": 1080, \"height\": 1920}\n",
    "        elif smartphone == \"S3Mini\":\n",
    "            pixels = {\"width\": 480, \"height\": 800}\n",
    "        elif smartphone == \"S4\":\n",
    "            pixels = {\"width\": 1080, \"height\": 1920}\n",
    "        elif smartphone == \"N6\":\n",
    "            pixels = {\"width\": 1440, \"height\": 2560}\n",
    "\n",
    "        # Read Files\n",
    "        raw_data = dict()\n",
    "\n",
    "        if output: print(\"read files\")\n",
    "        for file in file_names:\n",
    "            file_path = str(Path.home()) + \"/data/raw/fapra_imu-\" +  str(PID) + \"-\" + file + \"-\" + smartphone + \"-0.csv\"\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(file_path + \"not found\")\n",
    "                continue\n",
    "            raw_data[file] = pd.read_csv(file_path, \";\")\n",
    "\n",
    "        # split by time in seperate lists\n",
    "        if output: print(\"sort to specific interval\")\n",
    "        time_filtered_data = dict()\n",
    "        for name in file_names:\n",
    "            tmp = []\n",
    "            for k, end in enumerate(raw_data[\"points\"].time):\n",
    "                start = int(raw_data[\"fitts\"].time[k])\n",
    "                # include additional 150ms for smoothing\n",
    "                start = start - 150\n",
    "                # create mask for time interval\n",
    "                mask = (raw_data[name][\"time\"] > start) & (raw_data[name][\"time\"] <= end)\n",
    "                # only return items matching to mask\n",
    "                tmp.append(raw_data[name].loc[mask])\n",
    "            time_filtered_data[name] = tmp\n",
    "        # list to dataframe\n",
    "        time_filtered_data[\"points\"] = pd.concat(time_filtered_data[\"points\"])\n",
    "\n",
    "        # scale screen\n",
    "        if normalize_screen:\n",
    "            time_filtered_data[\"points\"][\"x-press\"] = time_filtered_data[\"points\"][\"x-press\"].div(pixels[\"width\"])\n",
    "            time_filtered_data[\"points\"][\"x-circle\"] = time_filtered_data[\"points\"][\"x-circle\"].div(pixels[\"width\"])\n",
    "            time_filtered_data[\"points\"][\"y-press\"] = time_filtered_data[\"points\"][\"y-press\"].div(pixels[\"height\"])\n",
    "            time_filtered_data[\"points\"][\"y-circle\"] = time_filtered_data[\"points\"][\"y-circle\"].div(pixels[\"height\"])\n",
    "\n",
    "        # filter unique timestamps\n",
    "        for k, item in enumerate(time_filtered_data[\"points\"]):\n",
    "            for sensor in sensors:\n",
    "                time_filtered_data[sensor][k] = time_filtered_data[sensor][k].drop_duplicates(subset=\"time\", keep=\"last\")\n",
    "        \n",
    "        # Create Array\n",
    "        result_interval = []\n",
    "        if output: print(\"create-interval\", end=' ')\n",
    "        for k, point in time_filtered_data[\"points\"].iterrows():\n",
    "            if output: print(k, end=' ', flush=True)\n",
    "            one_interval = []\n",
    "            interval = dict()\n",
    "            for sensor in sensors:\n",
    "                interval[sensor] = time_filtered_data[sensor][k]\n",
    "\n",
    "            # keeps index of each sensor\n",
    "            position = dict()\n",
    "            # keeps value of last sensor event\n",
    "            last_values = dict()\n",
    "            for sensor in sensors:\n",
    "                position[sensor] = 0\n",
    "                last_values[sensor] = 0\n",
    "\n",
    "            # find maximum first timestamp in all sensors\n",
    "            current_time = -1\n",
    "            for sensor in sensors:\n",
    "                if interval[sensor][\"time\"].iloc[position[sensor]] > current_time:\n",
    "                    current_time = interval[sensor][\"time\"].iloc[position[sensor]]\n",
    "                last_values[sensor] = interval[sensor][:].iloc[position[sensor]]\n",
    "            \n",
    "            # print(\"start\", raw_data[\"fitts\"].time.iloc[k])\n",
    "            # print(\"first_data\", current_time)\n",
    "            # print(\"end\", time_filtered_data[\"points\"].iloc[k][\"time\"])\n",
    "\n",
    "            # iterate as long as time has not reached max\n",
    "            while current_time <= time_filtered_data[\"points\"].iloc[k][\"time\"]:\n",
    "                # iterate over alle sensors to find values before current_time (maybe one sensor is having multiple updates)\n",
    "                while True:\n",
    "                    all_valid = True\n",
    "                    for sensor in sensors:\n",
    "                        if position[sensor] + 1 >= len(interval[sensor][\"time\"]):\n",
    "                            continue\n",
    "                        if interval[sensor][\"time\"].iloc[position[sensor] + 1] <= current_time:\n",
    "                            position[sensor] += 1\n",
    "                            last_values[sensor] = interval[sensor][:].iloc[position[sensor]]\n",
    "                            all_valid = False\n",
    "                    if all_valid:\n",
    "                        break\n",
    "                one_interval.append(last_values.copy())\n",
    "                current_time = current_time + event_time_diff\n",
    "            result_interval.append(one_interval)\n",
    "        \n",
    "        # make numpy arrays (without dicts)\n",
    "        if output: print()\n",
    "        if output: print(\"make numpy-array interval\", end=' ')\n",
    "        final_intervals = []\n",
    "        for k, interval in enumerate(result_interval):\n",
    "            if output: print(k, end=' ', flush=True)\n",
    "            current_interval = []\n",
    "            for i in interval:\n",
    "                event = []\n",
    "                for sensor in i.values():\n",
    "                    # except timestamp\n",
    "                    for value in sensor[1:]:\n",
    "                        event.append(value)\n",
    "                current_interval.append(np.asarray(event))\n",
    "            # make list to array, transpose and make 2d matrix\n",
    "            final_intervals.append(np.array(np.asarray(current_interval).transpose()))\n",
    "        \n",
    "        if output: print()\n",
    "        if output: print(\"make numpy-array points\")\n",
    "        final_points = time_filtered_data[\"points\"][['x-press','y-press','x-circle','y-circle']].values\n",
    "        final_result = [final_points, final_intervals]\n",
    "\n",
    "        # save dump pickles\n",
    "        data_path = str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(PID) + \"-\" + smartphone + \".pkl\"\n",
    "        pkl.dump(final_result, open( data_path, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for file-management\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6422a9b4ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/pickles/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../data/pickles/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for fn in os.listdir(\"../../data/pickles/\"):\n",
    "    if (fn.endswith(\".pkl\")):\n",
    "        file = \"../../data/pickles/\" + fn \n",
    "        data=np.load(file)    \n",
    "        df = pd.DataFrame(data[0], columns=[\"XTouch\", \"YTouch\", \"XTarget\", \"YTarget\"])\n",
    "        df[\"Participant\"] = [fn.split(\".\")[0].split(\"-\")[-2]] * len(df)\n",
    "        df[\"Phone\"] = [fn.split(\".\")[0].split(\"-\")[-1]] * len(df)\n",
    "        df[\"Sensor\"] = data[1]\n",
    "        lst.append(df)\n",
    "\n",
    "df = pd.concat(lst)\n",
    "df.Participant = df.Participant.astype(int)\n",
    "df.Sensor = df.Sensor.apply(lambda x: x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Length\"] = df.Sensor.apply(lambda x: x.shape[0])\n",
    "df.Length.describe()\n",
    "\n",
    "df[\"Input\"] = df.Sensor.apply(lambda x: x[5:58])\n",
    "x = df.Input.apply(lambda x: x.shape[0])\n",
    "\n",
    "df[[\"Participant\", \"Phone\", \"XTouch\", \"YTouch\", \"Input\"]].to_pickle(\"../../data/master_15ms.pkl\")\n",
    "\n",
    "print(data[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!EVERYTHING BELOW IS OLD!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for file-management\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = True\n",
    "participants = list(range(1, 21))\n",
    "smartphones = [\"S3Mini\", \"S4\", \"N5X\", \"N6\"]\n",
    "train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "test = [15, 16, 17, 18, 19, 20]\n",
    "window_sizes = [25, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdf-normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for window_size in window_sizes:\n",
    "    for smartphone in smartphones:\n",
    "        hf = h5py.File(str(Path.home()) + \"/data/hdf/\" + smartphone + \"-win\" + str(window_size) + \".hdf\", \"w\")\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            segments = list()\n",
    "            labels = list()\n",
    "            groups = list()\n",
    "            print(\"creating hdf5 - window_size:\", window_size, \"smartphone:\", smartphone, \"phase:\", phase, end=' ')\n",
    "            if output: print(\"participant:\", end=' ')\n",
    "            current_participants = train if phase is \"train\" else test\n",
    "            for pid in current_participants:\n",
    "                if output: print(pid, end=' ', flush=True)\n",
    "\n",
    "                data = pkl.load(open(str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(pid) + \"-\" + smartphone + \".pkl\", \"rb\"))\n",
    "                points = data[0]\n",
    "                intervals = data[1]\n",
    "                # split the points to according group\n",
    "                pressed = points[:,:2]\n",
    "                cross = points[:,2:]\n",
    "\n",
    "                # iterate over intervals\n",
    "                for i, c in zip(intervals, pressed):\n",
    "                    samples_per_interval = len(i[0]) - (window_size - 1)\n",
    "                    # create array of same label\n",
    "                    tmp_labels = [c] * samples_per_interval\n",
    "                    tmp_labels = np.array(tmp_labels)\n",
    "                    labels.append(tmp_labels)\n",
    "                    tmp_groups = [pid] * samples_per_interval\n",
    "                    groups = groups + tmp_groups\n",
    "                    for k in range(len(i.T[(window_size - 1):])):\n",
    "                        chunk = np.array(i.T[k:(k + window_size)])\n",
    "                        segments.append(chunk)\n",
    "\n",
    "            # make numpy arrays of it\n",
    "            # wenn window_size größer als intervall ist, dann wirf es ein error\n",
    "            labels = np.concatenate(np.array(labels), axis=0)\n",
    "            segments = np.array(segments)\n",
    "            groups = np.array(groups, dtype=np.int8)\n",
    "            \n",
    "            segments = segments.reshape(segments.shape[0], segments.shape[1], segments.shape[2], 1)\n",
    "\n",
    "            if phase is \"train\":\n",
    "                hf.create_dataset(\"train/labels\", data=labels)\n",
    "                hf.create_dataset(\"train/sensors\", data=segments)\n",
    "            else:\n",
    "                hf.create_dataset(\"test/labels\", data=labels)\n",
    "                hf.create_dataset(\"test/sensors\", data=segments)\n",
    "            print()\n",
    "        hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdf-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for window_size in window_sizes:\n",
    "    for smartphone in smartphones:\n",
    "        hf = h5py.File(str(Path.home()) + \"/data/hdf-small/\" + smartphone + \"-win\" + str(window_size) + \".hdf\", \"w\")\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            segments = list()\n",
    "            labels = list()\n",
    "            groups = list()\n",
    "            print(\"creating hdf5 - window_size:\", window_size, \"smartphone:\", smartphone, \"phase:\", phase, end=' ')\n",
    "            if output: print(\"participant:\", end=' ')\n",
    "            current_participants = train if phase is \"train\" else test\n",
    "            for pid in current_participants:\n",
    "                if output: print(pid, end=' ', flush=True)\n",
    "\n",
    "                data = pkl.load(open(str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(pid) + \"-\" + smartphone + \".pkl\", \"rb\"))\n",
    "                points = data[0]\n",
    "                intervals = data[1]\n",
    "                # split the points to according group\n",
    "                pressed = points[:,:2]\n",
    "                cross = points[:,2:]\n",
    "\n",
    "                # iterate over intervals\n",
    "                for i, c in zip(intervals, pressed):\n",
    "                    samples_per_interval = len(i[0]) - (window_size - 1)\n",
    "                    # create array of same label\n",
    "                    labels.append(np.array([c]))\n",
    "                    chunk = np.array(i.T[len(i.T[(window_size):]):(len(i.T[(window_size - 1):]) + window_size)])\n",
    "                    segments.append(chunk)\n",
    "\n",
    "            # make numpy arrays of it\n",
    "            # wenn window_size größer als intervall ist, dann wirf es ein error\n",
    "            labels = np.concatenate(np.array(labels), axis=0)\n",
    "            segments = np.array(segments)\n",
    "            \n",
    "            segments = segments.reshape(segments.shape[0], segments.shape[1], segments.shape[2], 1)\n",
    "\n",
    "            if phase is \"train\":\n",
    "                hf.create_dataset(\"train/labels\", data=labels)\n",
    "                hf.create_dataset(\"train/sensors\", data=segments)\n",
    "            else:\n",
    "                hf.create_dataset(\"test/labels\", data=labels)\n",
    "                hf.create_dataset(\"test/sensors\", data=segments)\n",
    "            print()\n",
    "        hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdf-less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for window_size in window_sizes:\n",
    "    for smartphone in smartphones:\n",
    "        hf = h5py.File(str(Path.home()) + \"/data/hdf-acc/\" + smartphone + \"-win\" + str(window_size) + \".hdf\", \"w\")\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            segments = list()\n",
    "            labels = list()\n",
    "            groups = list()\n",
    "            print(\"creating hdf5 - window_size:\", window_size, \"smartphone:\", smartphone, \"phase:\", phase, end=' ')\n",
    "            if output: print(\"participant:\", end=' ')\n",
    "            current_participants = train if phase is \"train\" else test\n",
    "            for pid in current_participants:\n",
    "                if output: print(pid, end=' ', flush=True)\n",
    "\n",
    "                data = pkl.load(open(str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(pid) + \"-\" + smartphone + \".pkl\", \"rb\"))\n",
    "                points = data[0]\n",
    "                intervals = data[1]\n",
    "                # split the points to according group\n",
    "                pressed = points[:,:2]\n",
    "                cross = points[:,2:]\n",
    "\n",
    "                # iterate over intervals\n",
    "                for i, c in zip(intervals, pressed):\n",
    "                    samples_per_interval = len(i[0]) - (window_size - 1)\n",
    "                    # create array of same label\n",
    "                    tmp_labels = [c] * samples_per_interval\n",
    "                    tmp_labels = np.array(tmp_labels)\n",
    "                    labels.append(tmp_labels)\n",
    "                    tmp_groups = [pid] * samples_per_interval\n",
    "                    groups = groups + tmp_groups\n",
    "                    for k in range(len(i.T[(window_size - 1):])):\n",
    "                        less_sensors = i.T[k:(k + window_size)].T[:3]\n",
    "                        chunk = np.array(less_sensors.T)\n",
    "                        segments.append(chunk)\n",
    "\n",
    "            # make numpy arrays of it\n",
    "            # wenn window_size größer als intervall ist, dann wirf es ein error\n",
    "            labels = np.concatenate(np.array(labels), axis=0)\n",
    "            segments = np.array(segments)\n",
    "            groups = np.array(groups, dtype=np.int8)\n",
    "            \n",
    "            segments = segments.reshape(segments.shape[0], segments.shape[1], segments.shape[2], 1)\n",
    "\n",
    "            hf.create_dataset(phase + \"/labels\", data=labels)\n",
    "            hf.create_dataset(phase + \"/sensors\", data=segments)\n",
    "            print()\n",
    "        hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
