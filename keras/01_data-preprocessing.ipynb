{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for file-management\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = False\n",
    "participants = list(range(1, 21))\n",
    "smartphones = [\"S3Mini\", \"S4\", \"N5X\", \"N6\"]\n",
    "# TODO remove\n",
    "participants = list(range(18, 19))\n",
    "event_time_diff = 3\n",
    "normalize_screen = False\n",
    "\n",
    "sensors = [\n",
    "    \"acc\",\n",
    "    \"gyro\",\n",
    "    \"ori\",\n",
    "    \"grav\",\n",
    "    \"mag\",\n",
    "    \"rot\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"points\",\n",
    "    \"fitts\"\n",
    "]\n",
    "\n",
    "file_names = sensors + tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing - participant: 7 smartphone: S3Mini\n",
      "processing - participant: 7 smartphone: S4\n",
      "processing - participant: 7 smartphone: N5X\n",
      "processing - participant: 7 smartphone: N6\n",
      "processing - participant: 8 smartphone: S3Mini\n",
      "processing - participant: 8 smartphone: S4\n",
      "processing - participant: 8 smartphone: N5X\n",
      "processing - participant: 8 smartphone: N6\n",
      "processing - participant: 9 smartphone: S3Mini\n",
      "processing - participant: 9 smartphone: S4\n",
      "processing - participant: 9 smartphone: N5X\n",
      "processing - participant: 9 smartphone: N6\n",
      "processing - participant: 10 smartphone: S3Mini\n",
      "processing - participant: 10 smartphone: S4\n",
      "processing - participant: 10 smartphone: N5X\n",
      "processing - participant: 10 smartphone: N6\n",
      "processing - participant: 11 smartphone: S3Mini\n",
      "processing - participant: 11 smartphone: S4\n",
      "processing - participant: 11 smartphone: N5X\n",
      "processing - participant: 11 smartphone: N6\n",
      "processing - participant: 12 smartphone: S3Mini\n",
      "processing - participant: 12 smartphone: S4\n",
      "processing - participant: 12 smartphone: N5X\n",
      "processing - participant: 12 smartphone: N6\n",
      "processing - participant: 13 smartphone: S3Mini\n",
      "processing - participant: 13 smartphone: S4\n",
      "processing - participant: 13 smartphone: N5X\n",
      "processing - participant: 13 smartphone: N6\n",
      "processing - participant: 14 smartphone: S3Mini\n",
      "processing - participant: 14 smartphone: S4\n",
      "processing - participant: 14 smartphone: N5X\n",
      "processing - participant: 14 smartphone: N6\n",
      "processing - participant: 15 smartphone: S3Mini\n",
      "processing - participant: 15 smartphone: S4\n",
      "processing - participant: 15 smartphone: N5X\n",
      "processing - participant: 15 smartphone: N6\n",
      "processing - participant: 16 smartphone: S3Mini\n",
      "processing - participant: 16 smartphone: S4\n",
      "processing - participant: 16 smartphone: N5X\n",
      "processing - participant: 16 smartphone: N6\n",
      "processing - participant: 17 smartphone: S3Mini\n",
      "processing - participant: 17 smartphone: S4\n",
      "processing - participant: 17 smartphone: N5X\n",
      "processing - participant: 17 smartphone: N6\n",
      "processing - participant: 18 smartphone: S3Mini\n",
      "processing - participant: 18 smartphone: S4\n",
      "processing - participant: 18 smartphone: N5X\n",
      "processing - participant: 18 smartphone: N6\n",
      "CPU times: user 4h 1min 24s, sys: 42.4 s, total: 4h 2min 7s\n",
      "Wall time: 4h 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for PID in participants:\n",
    "    for smartphone in smartphones:\n",
    "        print(\"processing - participant:\", str(PID), \"smartphone:\", smartphone)\n",
    "\n",
    "        # set screen resolution\n",
    "        if smartphone == \"N5X\":\n",
    "            pixels = {\"width\": 1080, \"height\": 1920}\n",
    "        elif smartphone == \"S3Mini\":\n",
    "            pixels = {\"width\": 480, \"height\": 800}\n",
    "        elif smartphone == \"S4\":\n",
    "            pixels = {\"width\": 1080, \"height\": 1920}\n",
    "        elif smartphone == \"N6\":\n",
    "            pixels = {\"width\": 1440, \"height\": 2560}\n",
    "\n",
    "        # Read Files\n",
    "        raw_data = dict()\n",
    "\n",
    "        if output: print(\"read files\")\n",
    "        for file in file_names:\n",
    "            file_path = str(Path.home()) + \"/data/raw/fapra_imu-\" +  str(PID) + \"-\" + file + \"-\" + smartphone + \"-0.csv\"\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(file_path + \"not found\")\n",
    "                continue\n",
    "            raw_data[file] = pd.read_csv(file_path, \";\")\n",
    "\n",
    "        # split by time in seperate lists\n",
    "        if output: print(\"sort to specific interval\")\n",
    "        time_filtered_data = dict()\n",
    "        for name in file_names:\n",
    "            tmp = []\n",
    "            for k, end in enumerate(raw_data[\"points\"].time):\n",
    "                start = int(raw_data[\"fitts\"].time[k])\n",
    "                # create mask for time interval\n",
    "                mask = (raw_data[name][\"time\"] > start) & (raw_data[name][\"time\"] <= end)\n",
    "                # only return items matching to mask\n",
    "                tmp.append(raw_data[name].loc[mask])\n",
    "            time_filtered_data[name] = tmp\n",
    "        # list to dataframe\n",
    "        time_filtered_data[\"points\"] = pd.concat(time_filtered_data[\"points\"])\n",
    "\n",
    "        # scale screen\n",
    "        if normalize_screen:\n",
    "            time_filtered_data[\"points\"][\"x-press\"] = time_filtered_data[\"points\"][\"x-press\"].div(pixels[\"width\"])\n",
    "            time_filtered_data[\"points\"][\"x-circle\"] = time_filtered_data[\"points\"][\"x-circle\"].div(pixels[\"width\"])\n",
    "            time_filtered_data[\"points\"][\"y-press\"] = time_filtered_data[\"points\"][\"y-press\"].div(pixels[\"height\"])\n",
    "            time_filtered_data[\"points\"][\"y-circle\"] = time_filtered_data[\"points\"][\"y-circle\"].div(pixels[\"height\"])\n",
    "\n",
    "        # filter unique timestamps\n",
    "        for k, item in enumerate(time_filtered_data[\"points\"]):\n",
    "            for sensor in sensors:\n",
    "                time_filtered_data[sensor][k] = time_filtered_data[sensor][k].drop_duplicates(subset=\"time\", keep=\"last\")\n",
    "        \n",
    "        # Create Array\n",
    "        result_interval = []\n",
    "        if output: print(\"create-interval\", end=' ')\n",
    "        for k, point in time_filtered_data[\"points\"].iterrows():\n",
    "            if output: print(k, end=' ', flush=True)\n",
    "            one_interval = []\n",
    "            interval = dict()\n",
    "            for sensor in sensors:\n",
    "                interval[sensor] = time_filtered_data[sensor][k]\n",
    "\n",
    "            # keeps index of each sensor\n",
    "            position = dict()\n",
    "            # keeps value of last sensor event\n",
    "            last_values = dict()\n",
    "            for sensor in sensors:\n",
    "                position[sensor] = 0\n",
    "                last_values[sensor] = 0\n",
    "\n",
    "            # find maximum first timestamp in all sensors\n",
    "            current_time = -1\n",
    "            for sensor in sensors:\n",
    "                if interval[sensor][\"time\"].iloc[position[sensor]] > current_time:\n",
    "                    current_time = interval[sensor][\"time\"].iloc[position[sensor]]\n",
    "                last_values[sensor] = interval[sensor][:].iloc[position[sensor]]\n",
    "            \n",
    "            # print(\"start\", raw_data[\"fitts\"].time.iloc[k])\n",
    "            # print(\"first_data\", current_time)\n",
    "            # print(\"end\", time_filtered_data[\"points\"].iloc[k][\"time\"])\n",
    "\n",
    "            # iterate as long as time has not reached max\n",
    "            while current_time <= time_filtered_data[\"points\"].iloc[k][\"time\"]:\n",
    "                # iterate over alle sensors to find values before current_time (maybe one sensor is having multiple updates)\n",
    "                while True:\n",
    "                    all_valid = True\n",
    "                    for sensor in sensors:\n",
    "                        if position[sensor] + 1 >= len(interval[sensor][\"time\"]):\n",
    "                            continue\n",
    "                        if interval[sensor][\"time\"].iloc[position[sensor] + 1] <= current_time:\n",
    "                            position[sensor] += 1\n",
    "                            last_values[sensor] = interval[sensor][:].iloc[position[sensor]]\n",
    "                            all_valid = False\n",
    "                    if all_valid:\n",
    "                        break\n",
    "                one_interval.append(last_values.copy())\n",
    "                current_time = current_time + event_time_diff\n",
    "            result_interval.append(one_interval)\n",
    "        \n",
    "        # make numpy arrays (without dicts)\n",
    "        if output: print()\n",
    "        if output: print(\"make numpy-array interval\", end=' ')\n",
    "        final_intervals = []\n",
    "        for k, interval in enumerate(result_interval):\n",
    "            if output: print(k, end=' ', flush=True)\n",
    "            current_interval = []\n",
    "            for i in interval:\n",
    "                event = []\n",
    "                for sensor in i.values():\n",
    "                    # except timestamp\n",
    "                    for value in sensor[1:]:\n",
    "                        event.append(value)\n",
    "                current_interval.append(np.asarray(event))\n",
    "            # make list to array, transpose and make 2d matrix\n",
    "            final_intervals.append(np.array(np.asarray(current_interval).transpose()))\n",
    "        \n",
    "        if output: print()\n",
    "        if output: print(\"make numpy-array points\")\n",
    "        final_points = time_filtered_data[\"points\"][['x-press','x-circle','y-press','y-circle']].values\n",
    "        final_result = [final_points, final_intervals]\n",
    "\n",
    "        # save dump pickles\n",
    "        data_path = str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(PID) + \"-\" + smartphone + \".pkl\"\n",
    "        pkl.dump(final_result, open( data_path, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for file-management\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = True\n",
    "participants = list(range(1, 21))\n",
    "smartphones = [\"S3Mini\", \"S4\", \"N5X\", \"N6\"]\n",
    "# TODO remove\n",
    "participants = list(range(1, 19))\n",
    "train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "test = [14, 15, 16, 17, 18]\n",
    "window_sizes = [1, 10, 25, 50, 75]# , 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating hdf5 - window_size: 1 smartphone: S3Mini phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 1 smartphone: S3Mini phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 1 smartphone: S4 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 1 smartphone: S4 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 1 smartphone: N5X phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 1 smartphone: N5X phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 1 smartphone: N6 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 1 smartphone: N6 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 10 smartphone: S3Mini phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 10 smartphone: S3Mini phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 10 smartphone: S4 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 10 smartphone: S4 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 10 smartphone: N5X phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 10 smartphone: N5X phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 10 smartphone: N6 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 10 smartphone: N6 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 25 smartphone: S3Mini phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 25 smartphone: S3Mini phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 25 smartphone: S4 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 25 smartphone: S4 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 25 smartphone: N5X phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 25 smartphone: N5X phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 25 smartphone: N6 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 25 smartphone: N6 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 50 smartphone: S3Mini phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 50 smartphone: S3Mini phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 50 smartphone: S4 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 50 smartphone: S4 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 50 smartphone: N5X phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 50 smartphone: N5X phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 50 smartphone: N6 phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "creating hdf5 - window_size: 50 smartphone: N6 phase: test participant: 14 15 16 17 18 \n",
      "creating hdf5 - window_size: 75 smartphone: S3Mini phase: train participant: 1 2 3 4 5 6 7 8 9 10 11 12 13 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for window_size in window_sizes:\n",
    "    for smartphone in smartphones:\n",
    "        hf = h5py.File(str(Path.home()) + \"/data/hdf/\" + smartphone + \"-win\" + str(window_size) + \".hdf\", \"w\")\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            segments = list()\n",
    "            labels = list()\n",
    "            groups = list()\n",
    "            print(\"creating hdf5 - window_size:\", window_size, \"smartphone:\", smartphone, \"phase:\", phase, end=' ')\n",
    "            if output: print(\"participant:\", end=' ')\n",
    "            current_participants = train if phase is \"train\" else test\n",
    "            for pid in current_participants:\n",
    "                if output: print(pid, end=' ', flush=True)\n",
    "\n",
    "                data = pkl.load(open(str(Path.home()) + \"/data/pickles/fapra_imu-processed-\" +  str(pid) + \"-\" + smartphone + \".pkl\", \"rb\"))\n",
    "                points = data[0]\n",
    "                intervals = data[1]\n",
    "                # split the points to according group\n",
    "                cross = points[:,:2]\n",
    "                pressed = points[:,2:]\n",
    "\n",
    "                # iterate over intervals\n",
    "                for i, c in zip(intervals, pressed):\n",
    "                    samples_per_interval = len(i[0]) - (window_size - 1)\n",
    "                    # create array of same label\n",
    "                    tmp_labels = [c] * samples_per_interval\n",
    "                    tmp_labels = np.array(tmp_labels)\n",
    "                    labels.append(tmp_labels)\n",
    "                    tmp_groups = [pid] * samples_per_interval\n",
    "                    groups = groups + tmp_groups\n",
    "                    for k in range(len(i.T[(window_size - 1):])):\n",
    "                        chunk = np.array(i.T[k:(k + window_size)])\n",
    "                        segments.append(chunk)\n",
    "\n",
    "            # make numpy arrays of it\n",
    "            labels = np.concatenate(np.array(labels))\n",
    "            segments = np.array(segments)\n",
    "            groups = np.array(groups, dtype=np.int8)\n",
    "            \n",
    "            segments = segments.reshape(segments.shape[0], segments.shape[1], segments.shape[2], 1)\n",
    "\n",
    "            if phase is \"train\":\n",
    "                hf.create_dataset(\"train/labels\", data=labels)\n",
    "                hf.create_dataset(\"train/sensors\", data=segments)\n",
    "            else:\n",
    "                hf.create_dataset(\"test/labels\", data=labels)\n",
    "                hf.create_dataset(\"test/sensors\", data=segments)\n",
    "            print()\n",
    "        hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
