\section{Related Work}
\label{sec:relw}
This paper explores the applicability of touch prediction on smartphones with built-in sensors. 
We considered related work that extends or improves everyday interaction and therefore looked into work that tries to adapt touch interaction in the field of mobile interaction.

An addition to the interaction enhancement through e.g. BoD elements could be the integration of the internal sensors of smartphones, as modern telephones are shipped with them by default.
\citeauthor{Goel2012a} \cite{Goel2012a} presented \textit{GripSense}, a system that can imply pressure and infers hand postures on phones based on inertial sensor measurements by the gyroscope with an accuracy of 84.3\%.
To reduce the noticeable latency of continuous motion on touchscreens, \citeauthor{Le2017:Predic} \cite{Le2017:Predic} introduced \textit{PredicTouch}, a system consisting of three external IMUs attached to the wrist, the finger, and to a stylus in order to predict where users will continue their motion on touchscreens in the near future.
Using a combination of IMU and a multi-layer feedforward neural network for regression, which was trained on touch coordinates with preceding data from the IMU, they were able to accurately predict touches 33ms and 66ms into the future.
Additionally, user's throughput for finger input was increased by 15\% and 17\% for stylus input.
Support was found for \citeauthor{Hinckley2016} \cite{Hinckley2016} approach of using self-capacitance touchscreen displays as a means for mobile interaction.
Self-capacitive touchscreens capture fingers and their distance from the screen before touching it.  
Their contribution covers the applicability of multi-touch hover and grip, which is enabled by self-capacitance touchscreens, in common interaction scenarios.
Motivated by overcoming the lack of input modalities, \citeauthor{MohdNoor2016} \cite{MohdNoor2016} presented \textit{28 Frames Later}, a system that predicts future touch positions on smartphones.
Based on grip data they gathered from a total of 24 capacitive sensors built inside the BoD and on the laterals while performing touches on the touchscreen they built a machine learning model that was able to predict touch positions $ 200ms $ before the actual touch with an offset of $ 18mm $ to the actual touch position.
However their system required the 24 built on sensors which is not feasible for ordinary usage.

Some of the presented work require additional features for their approach to work.
This includes, for example, having additional sensors mounted to the telephone or having special touch-sensitive displays. 
The normal use of today's smartphones does not support this kind of interaction elements.
However, the use of internal sensors is a promising solution for this limitation as smartphones nowadays are equipped with them as standard.
Confirmation of this can be found in the approaches mentioned above.
We present a combination of a neural network in combination with the internal sensors to predict where one will touch in the near future. 
